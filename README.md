# image-related
This is my image tools library, all codes here are developed by myself, uses only some basic python modules. Most of the time I used them in my routine work to get some statistics for a given image. This encludes the following:
1. greedyBin.py,  a python class that reduces the complexity of predictive models. A predictive models is oftenly implemented as a mix of distributions P(x|i), i=0,1,...N where x is the random variable, and i is a state that depends on some statistics (for example, it can be the output of a function f(x)). greedyBin uses a greedy algorithm to search through i, binning the two most similar distributions each iteration. In the given demo, a set of Gaussian N(mean, std) is generated, with N setting of means and stds. The running result showes that this algorithm can pick up distributions that comes from the similar means and stds.
2. analyzer.py, a framework that extracts words from unstructured data, and create "finger prints" for the data. Then different blocks of data can be compared by their fingerprints using methods such as minHash algorithm. 
3. lip_png.py, png_decode.py: a manual png (portable network graphic) format decoder, can decode several most common png type by now (currently only true color mode, paltte mode on the way). This is a project motivated by the need of finding customized solution under png's framework, as it is simply, and uses un-patent DEFLATE codec (lz77+Huffman, implemented already in the zlib standard library). png is still a very good lossless format with many artificial images (unlike realwork photo with a lot of noise). The new Webp format did not do more than extending png's idea to more complex filter and more efficient coding methods, I believe a better solution can be obtained using machine learning. 
